{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fossil-mainstream",
   "metadata": {},
   "source": [
    "# PRAD : Analysis between risk-groups \n",
    "\n",
    "# Introduction\n",
    "\n",
    "We will find the differentially expressed genes between predicted high and low risk groups. \n",
    "\n",
    "# Preparing workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intimate-salvation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.7     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.9\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.2     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Loading required package: ggpubr\n",
      "\n",
      "\n",
      "Attaching package: ‘survminer’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:survival’:\n",
      "\n",
      "    myeloma\n",
      "\n",
      "\n",
      "Loading required package: Matrix\n",
      "\n",
      "\n",
      "Attaching package: ‘Matrix’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:tidyr’:\n",
      "\n",
      "    expand, pack, unpack\n",
      "\n",
      "\n",
      "Loaded glmnet 4.1-4\n",
      "\n",
      "========================================\n",
      "circlize version 0.4.14\n",
      "CRAN page: https://cran.r-project.org/package=circlize\n",
      "Github page: https://github.com/jokergoo/circlize\n",
      "Documentation: https://jokergoo.github.io/circlize_book/book/\n",
      "\n",
      "If you use it in published research, please cite:\n",
      "Gu, Z. circlize implements and enhances circular visualization\n",
      "  in R. Bioinformatics 2014.\n",
      "\n",
      "This message can be suppressed by:\n",
      "  suppressPackageStartupMessages(library(circlize))\n",
      "========================================\n",
      "\n",
      "\n",
      "Loading required package: grid\n",
      "\n",
      "========================================\n",
      "ComplexHeatmap version 2.10.0\n",
      "Bioconductor page: http://bioconductor.org/packages/ComplexHeatmap/\n",
      "Github page: https://github.com/jokergoo/ComplexHeatmap\n",
      "Documentation: http://jokergoo.github.io/ComplexHeatmap-reference\n",
      "\n",
      "If you use it in published research, please cite:\n",
      "Gu, Z. Complex heatmaps reveal patterns and correlations in multidimensional \n",
      "  genomic data. Bioinformatics 2016.\n",
      "\n",
      "The new InteractiveComplexHeatmap package can directly export static \n",
      "complex heatmaps into an interactive Shiny app with zero effort. Have a try!\n",
      "\n",
      "This message can be suppressed by:\n",
      "  suppressPackageStartupMessages(library(ComplexHeatmap))\n",
      "========================================\n",
      "\n",
      "\n",
      "Loading required package: prodlim\n",
      "\n",
      "Loading required package: S4Vectors\n",
      "\n",
      "Loading required package: stats4\n",
      "\n",
      "Loading required package: BiocGenerics\n",
      "\n",
      "\n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    combine, intersect, setdiff, union\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, append, as.data.frame, basename, cbind, colnames,\n",
      "    dirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n",
      "    grepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\n",
      "    order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n",
      "    rbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\n",
      "    union, unique, unsplit, which.max, which.min\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:Matrix’:\n",
      "\n",
      "    expand, unname\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    first, rename\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    expand\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    expand.grid, I, unname\n",
      "\n",
      "\n",
      "Loading required package: IRanges\n",
      "\n",
      "\n",
      "Attaching package: ‘IRanges’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    collapse, desc, slice\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    reduce\n",
      "\n",
      "\n",
      "Loading required package: GenomicRanges\n",
      "\n",
      "Loading required package: GenomeInfoDb\n",
      "\n",
      "Loading required package: SummarizedExperiment\n",
      "\n",
      "Loading required package: MatrixGenerics\n",
      "\n",
      "Loading required package: matrixStats\n",
      "\n",
      "\n",
      "Attaching package: ‘matrixStats’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    count\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘MatrixGenerics’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n",
      "    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n",
      "    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n",
      "    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n",
      "    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n",
      "    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n",
      "    colWeightedMeans, colWeightedMedians, colWeightedSds,\n",
      "    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n",
      "    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n",
      "    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n",
      "    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n",
      "    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n",
      "    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n",
      "    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n",
      "    rowWeightedSds, rowWeightedVars\n",
      "\n",
      "\n",
      "Loading required package: Biobase\n",
      "\n",
      "Welcome to Bioconductor\n",
      "\n",
      "    Vignettes contain introductory material; view with\n",
      "    'browseVignettes()'. To cite Bioconductor, see\n",
      "    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘Biobase’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:MatrixGenerics’:\n",
      "\n",
      "    rowMedians\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    anyMissing, rowMedians\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setwd(\"/home/data/project_code/landstrom_project_3/r/notebooks\")\n",
    "\n",
    "library(ggplot2)\n",
    "library(tidyverse)\n",
    "library(survival)\n",
    "library(survminer)\n",
    "library(glmnet)\n",
    "library(WriteXLS)\n",
    "library(ggfortify)\n",
    "library(circlize)\n",
    "library(ComplexHeatmap)\n",
    "library(parallel)\n",
    "library(broom)\n",
    "library(survcomp)\n",
    "library(survivalROC)\n",
    "source(\"../getTCGAData.R\")\n",
    "source(\"../preprocessTCGAData.R\")\n",
    "source(\"../KM_analysis.R\")\n",
    "source(\"../Heatmaps.R\")\n",
    "source(\"../enet.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-costume",
   "metadata": {},
   "source": [
    "# Setting up paths and clinical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moral-progressive",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the cancer type \n",
    "cancer.type = \"PRAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radio-majority",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the table including the clinical features for each cancer type\n",
    "clin.feat.tb = read.table(\"/workstation/project_data/landstrom_project_3/clin_features_final.csv\", sep = \"\\t\", header = T)\n",
    "\n",
    "# Get Clinical variables\n",
    "clin.var = unlist(strsplit(clin.feat.tb$Features[clin.feat.tb$Ctype == cancer.type], split = \",\"))\n",
    "\n",
    "# Ensembl id mapping file \n",
    "ens.id.mapping = \"/home/organisms/Human/hg38/Homo_sapiens.GRCh38_March2022/ENSEMBLE_to_SYMBOL.csv\"\n",
    "\n",
    "# Output dir \n",
    "out.dir.data = file.path(\"/workstation/project_data/landstrom_core/rdata/manuscript_work/\", cancer.type)\n",
    "dir.create(out.dir.data, recursive = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-literacy",
   "metadata": {},
   "source": [
    "# 1. KM based univariate feature selection\n",
    "\n",
    "We will now perform the univariate feature selection which is the first phase of \n",
    "the actual analysis. The idea is to prefilter some features which have no predictive \n",
    "power regarding survival. We will select one clinical end point which seems to carry \n",
    "most events to maximise the statistical power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-horror",
   "metadata": {},
   "source": [
    "## 1.1 Loading data and selection of variables \n",
    "\n",
    "Load the dataset if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "compressed-incident",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the preprocessed dataset if continued \n",
    "tcga.dataset = readRDS(file.path(out.dir.data, \"tcga.dataset.rds\"))\n",
    "\n",
    "# Raw expression data \n",
    "tcga.expr.raw.datamat = readRDS(file.path(out.dir.data, \"raw_expressions.rds\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-printer",
   "metadata": {},
   "source": [
    "Define and create output directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "enclosed-windsor",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(dir.res.root, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD' already exists”\n",
      "Warning message in dir.create(dir.res.km):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots' already exists”\n"
     ]
    }
   ],
   "source": [
    "# Define and create the root directory for results \n",
    "dir.res.root = file.path(\"/workstation/project_results/landstrom_project_3/manuscript_work/\", cancer.type)\n",
    "dir.create(dir.res.root, recursive = T)\n",
    "\n",
    "# Define and create the results for the KM analysis \n",
    "dir.res.km = file.path(dir.res.root, \"Kaplan_Meier_plots\")\n",
    "dir.create(dir.res.km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-newport",
   "metadata": {},
   "source": [
    "Read in the gene list of interest including the customer genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "egyptian-twelve",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gene list  \n",
    "gene.list.file = read.table(\"/workstation/project_data/landstrom_core/Customer_genes.tsv\", \n",
    "                            sep = \"\\t\", header = F)\n",
    "gene.list = gene.list.file$V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-thinking",
   "metadata": {},
   "source": [
    "Tabulate the number of events. Value 0 means sensored and value 1 an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "chicken-toronto",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'name'. You can override using the `.groups` argument.\n"
     ]
    }
   ],
   "source": [
    "clinical.end.point.stats = tcga.dataset %>% \n",
    "                                   dplyr::select(c(\"OS.clin\",\"DSS.clin\",\"DFI.clin\",\"PFI.clin\")) %>%\n",
    "                                   pivot_longer(everything()) %>%\n",
    "                                   mutate(value = factor(value)) %>%\n",
    "                                   group_by(name, value) %>%\n",
    "                                   summarise(N = n()) %>% \n",
    "                                   pivot_wider(names_from =  value,\n",
    "                                               values_from = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ambient-beatles",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 4 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>name</th><th scope=col>0</th><th scope=col>1</th><th scope=col>NA</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>DFI.clin</td><td>310</td><td>30</td><td>160</td></tr>\n",
       "\t<tr><td>DSS.clin</td><td>493</td><td> 5</td><td>  2</td></tr>\n",
       "\t<tr><td>OS.clin </td><td>490</td><td>10</td><td> NA</td></tr>\n",
       "\t<tr><td>PFI.clin</td><td>407</td><td>93</td><td> NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 4 × 4\n",
       "\\begin{tabular}{llll}\n",
       " name & 0 & 1 & NA\\\\\n",
       " <chr> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t DFI.clin & 310 & 30 & 160\\\\\n",
       "\t DSS.clin & 493 &  5 &   2\\\\\n",
       "\t OS.clin  & 490 & 10 &  NA\\\\\n",
       "\t PFI.clin & 407 & 93 &  NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 4 × 4\n",
       "\n",
       "| name &lt;chr&gt; | 0 &lt;int&gt; | 1 &lt;int&gt; | NA &lt;int&gt; |\n",
       "|---|---|---|---|\n",
       "| DFI.clin | 310 | 30 | 160 |\n",
       "| DSS.clin | 493 |  5 |   2 |\n",
       "| OS.clin  | 490 | 10 |  NA |\n",
       "| PFI.clin | 407 | 93 |  NA |\n",
       "\n"
      ],
      "text/plain": [
       "  name     0   1  NA \n",
       "1 DFI.clin 310 30 160\n",
       "2 DSS.clin 493  5   2\n",
       "3 OS.clin  490 10  NA\n",
       "4 PFI.clin 407 93  NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clinical.end.point.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-reason",
   "metadata": {},
   "source": [
    "## 1.2 Splitting dataset into training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-reputation",
   "metadata": {},
   "source": [
    "Here we change the original workflow such that we run the analysis for all clinical end points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "comparable-journalism",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we store all the training and validation splits \n",
    "train_and_validation_ls = list()\n",
    "\n",
    "# Variables selected \n",
    "variables_selected_ls = list()\n",
    "\n",
    "# Number of samples in training and validation cohorts \n",
    "nsamples_step1_ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "metric-floor",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 492 cases out of 500 cases\"\n",
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 491 cases out of 500 cases\"\n",
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 336 cases out of 500 cases\"\n",
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 492 cases out of 500 cases\"\n"
     ]
    }
   ],
   "source": [
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "\n",
    "    # Selected variables \n",
    "    variables.selected = selectVariables(clinical.endpoint = end.point, \n",
    "                                     gene.list = gene.list, \n",
    "                                     data.suffixes = c(\"cn\",\"exp\"))\n",
    "    variables_selected_ls[[end.point]] = variables.selected\n",
    "    \n",
    "    #Data set is split randomly into training and validation sets. Only complete cases \n",
    "    # are selected.\n",
    "    train_and_validation = splitCases(data = tcga.dataset, \n",
    "                                  split = 0.75, \n",
    "                                  only.complete = T,\n",
    "                                  variables = variables.selected, \n",
    "                                  seed = 42)\n",
    "    \n",
    "    # Update list\n",
    "    train_and_validation_ls[[end.point]] = train_and_validation \n",
    "    \n",
    "    \n",
    "    # Store number of  \n",
    "    nsamples.step1 = c(nrow(train_and_validation$train), nrow(train_and_validation$validation))\n",
    "    names(nsamples.step1) = c(\"ntrain.step1\", \"nvalid.step1\")\n",
    "    nsamples_step1_ls[[end.point]] = nsamples.step1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-professional",
   "metadata": {},
   "source": [
    "## 1.3 Filtering step "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-piano",
   "metadata": {},
   "source": [
    "### 1.3.1 Calculate relevant statistics for the training set. \n",
    "\n",
    "We will calculate the following statistics for expression features based on the raw expression data : \n",
    "1. The fraction of individuals expressing the feature  \n",
    "2. Median expression of the feature\n",
    "3. Variance of the feature\n",
    "\n",
    "We will calculate the following statistics for copy number features \n",
    "1. Fraction of individuals with amplification \n",
    "2. Fraction of individuals with deletion\n",
    "3. Fraction of individuals with missing CN status \n",
    "4. Max value out of the fraction of individuals with amplification and deletions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fifty-shift",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store summary statistics \n",
    "summary.stats.ls = list()\n",
    "\n",
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    exp.summary.training = prepSummaryExp(x = train_and_validation$train, \n",
    "                                      raw.data = tcga.expr.raw.datamat,\n",
    "                                      variables = variables.selected, type = \"exp\")\n",
    "\n",
    "    cn.summary.training = prepSummaryCN(train_and_validation$train, variables = variables.selected, type = \"cn\")\n",
    "    \n",
    "    summary.stats.ls[[end.point]] = list(\"exp.summary.training\" = exp.summary.training,\n",
    "                                         \"cn.summary.training\" = cn.summary.training)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-denver",
   "metadata": {},
   "source": [
    "### 1.3.2 Filter based on the calculated statistics\n",
    "\n",
    "We will set the filtering thresholds as follows : \n",
    "\n",
    "Expression features : \n",
    "\n",
    "1. Median expression must be greater than 20\n",
    "2. Fraction of individuals expressing feature must be greater than > 0.25\n",
    "\n",
    "CN features \n",
    "\n",
    "1. Maximum Fraction of individuals carrying either deletion or amplification must be at least 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dried-panic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store filtered variables \n",
    "variables.selected.filtered.ls = list()\n",
    "\n",
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "\n",
    "    exp.features.keep = summary.stats.ls[[end.point]]$exp.summary.training %>% \n",
    "                          filter(`Median expression` > 20, \n",
    "                                 `Fraction of zero expression` < 0.75)\n",
    "\n",
    "    cn.features.keep = summary.stats.ls[[end.point]]$cn.summary.training %>% \n",
    "                          filter(`Maximum fraction of aberrations` > 0.15) \n",
    "\n",
    "    # Update the summary tables \n",
    "    summary.stats.ls[[end.point]]$exp.summary.training$Selected = ifelse(summary.stats.ls[[end.point]]$exp.summary.training$name %in% exp.features.keep$name, \"Yes\", \"No\")\n",
    "    summary.stats.ls[[end.point]]$cn.summary.training$Selected = ifelse(summary.stats.ls[[end.point]]$cn.summary.training$name %in% cn.features.keep$name, \"Yes\", \"No\")\n",
    "\n",
    "    # Collect the variables into vector \n",
    "    variables.selected.filtered.ls[[end.point]] = filterFeatures(variables_selected_ls[[end.point]], exp.features.keep$name, type = \"exp\")\n",
    "    variables.selected.filtered.ls[[end.point]]= filterFeatures(variables_selected_ls[[end.point]], cn.features.keep$name, type = \"cn\")\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-guest",
   "metadata": {},
   "source": [
    "Final list of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-investigator",
   "metadata": {},
   "source": [
    "## 3.4 Prepare univariate KM plots and logrank tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "indian-tribute",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(file.path(dir.res.km, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/OS' already exists”\n",
      "Warning message in dir.create(out.exp.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/OS/Expression/Training_data' already exists”\n",
      "Warning message in dir.create(out.exp.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/OS/Expression/Validation_data' already exists”\n",
      "Warning message in dir.create(out.cn.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/OS/CN/Training_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n",
      "NULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(out.cn.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/OS/CN/Validation_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(file.path(dir.res.km, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DSS' already exists”\n",
      "Warning message in dir.create(out.exp.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DSS/Expression/Training_data' already exists”\n",
      "Warning message in dir.create(out.exp.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DSS/Expression/Validation_data' already exists”\n",
      "Warning message in dir.create(out.cn.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DSS/CN/Training_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n",
      "NULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(out.cn.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DSS/CN/Validation_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(file.path(dir.res.km, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DFI' already exists”\n",
      "Warning message in dir.create(out.exp.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DFI/Expression/Training_data' already exists”\n",
      "Warning message in dir.create(out.exp.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DFI/Expression/Validation_data' already exists”\n",
      "Warning message in dir.create(out.cn.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DFI/CN/Training_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(out.cn.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/DFI/CN/Validation_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n",
      "NULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(file.path(dir.res.km, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/PFI' already exists”\n",
      "Warning message in dir.create(out.exp.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/PFI/Expression/Training_data' already exists”\n",
      "Warning message in dir.create(out.exp.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/PFI/Expression/Validation_data' already exists”\n",
      "Warning message in dir.create(out.cn.train.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/PFI/CN/Training_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n",
      "NULL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(out.cn.valid.dir, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Kaplan_Meier_plots/PFI/CN/Validation_data' already exists”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL\n"
     ]
    }
   ],
   "source": [
    "# Store the KM tables \n",
    "km.pvalue.table.ls = list()\n",
    "\n",
    "# Store the significant features \n",
    "significant.features.ls = list()\n",
    "\n",
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "\n",
    "    # Create dir for plots \n",
    "    dir.create(file.path(dir.res.km, end.point))\n",
    "    \n",
    "    if (nrow(train_and_validation_ls[[end.point]]$train) > 0){\n",
    "    \n",
    "        # Run univariate KM\n",
    "        km.pvalue.table = runUnivariateKM(input.data = train_and_validation_ls[[end.point]], \n",
    "                                  variables = variables.selected.filtered.ls[[end.point]],\n",
    "                                  clinical.endpoint = end.point,\n",
    "                                  out.dir = file.path(dir.res.km, end.point),\n",
    "                                  plots = T)\n",
    "    \n",
    "    \n",
    "        # Sort the results based on the training p-value and write the results to output\n",
    "        km.pvalue.table = km.pvalue.table %>% dplyr::arrange(pvalues.training)\n",
    "        km.pvalue.table$Selected = ifelse(km.pvalue.table$pvalues.training < 0.05, \"Yes\", \"No\") \n",
    "        write.csv(km.pvalue.table, file.path(dir.res.km, paste0(end.point, \"_LogRank_pvalues.csv\")))\n",
    "    \n",
    "        km.pvalue.table.ls[[end.point]] = km.pvalue.table\n",
    "    \n",
    "        # Extract the significant features \n",
    "        significant.features = getSignificantFeatures(km.pvalue.table, pvalue.thresh = 0.05)\n",
    "\n",
    "        # Store \n",
    "        significant.features.ls[[end.point]] = significant.features\n",
    "        \n",
    "    } else {\n",
    "        significant.features.ls[[end.point]] = NULL\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314db78-7ceb-4fed-b4a9-6daf4a680747",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant.features.ls[[end.point]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-habitat",
   "metadata": {},
   "source": [
    "# 2 Penalised cox regression without clinical variables \n",
    "\n",
    "Define path to output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "regular-athens",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(dir.res.pcox, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/Without_clinical_features' already exists”\n"
     ]
    }
   ],
   "source": [
    "dir.res.pcox = file.path(dir.res.root, \"Penalized_Cox_risk_prediction/customer_features/Without_clinical_features\")\n",
    "dir.create(dir.res.pcox, recursive = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "following-damage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function for fixing variable names \n",
    "fixVarNames = function(x){\n",
    "    if (str_detect(x, \"Gender\")) {\n",
    "        return(\"Gender\")\n",
    "    } else if (str_detect(x, \"Tumor.stage\")){\n",
    "        return(\"Tumor.stage\")\n",
    "    } else if (str_detect(x,\".cn\")){\n",
    "        return(str_extract(x, \"\\\\w+.cn\"))\n",
    "    } else if (str_detect(x, \"Gleason.group\")){ \n",
    "        return(\"Gleason.group\")\n",
    "    } else {\n",
    "        return(x)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-suicide",
   "metadata": {},
   "source": [
    "## 2.1 Splitting dataset into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ethical-conditioning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we store all the training and validation splits \n",
    "train_and_validation_ls = list()\n",
    "\n",
    "# Number of samples in training and validation cohorts \n",
    "nsamples_step2_ls_no_clin = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "civilian-algebra",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 493 cases out of 500 cases\"\n",
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 337 cases out of 500 cases\"\n",
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 492 cases out of 500 cases\"\n"
     ]
    }
   ],
   "source": [
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    # We will first combine the significant.features with the outcome variables\n",
    "    # Final list of features \n",
    "    feature.ls = c(paste0(end.point, c(\".clin\",\".time.clin\")), significant.features.ls[[end.point]])\n",
    "    \n",
    "    if (is.null(significant.features.ls[[end.point]]) == F) {\n",
    "    \n",
    "        # Now we split the dataset into training and validation cohorts exactly as before.\n",
    "        train_and_validation = splitCases(data = tcga.dataset, \n",
    "                                  split = 0.75, \n",
    "                                  only.complete = T,\n",
    "                                  variables = feature.ls,\n",
    "                                  seed = 42)\n",
    "    \n",
    "        # Store \n",
    "        train_and_validation_ls[[end.point]] = train_and_validation\n",
    "    \n",
    "        # Store the number of samples       \n",
    "        nsamples.step2 = c(nrow(train_and_validation$train), nrow(train_and_validation$validation))\n",
    "    }    \n",
    "    else {\n",
    "    \n",
    "        # If there are now significant features store NULL\n",
    "        \n",
    "        # Store \n",
    "        train_and_validation_ls[[end.point]] = NULL\n",
    "        nsamples.step2 = c(NA, NA)\n",
    "    }\n",
    "        \n",
    "    names(nsamples.step2) = c(\"ntrain.step2\", \"nvalid.step2\")\n",
    "    nsamples_step2_ls_no_clin[[end.point]] = nsamples.step2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-choir",
   "metadata": {},
   "source": [
    "## 2.2 Find the optimal lambda \n",
    "\n",
    "Use 10-fold cross-validation (CV) for the Cox model for different values of lamda. C-index will be use to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "veterinary-holiday",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store significant features \n",
    "rcox.res.no.clin.ls = list()\n",
    "\n",
    "# Store model matrices\n",
    "model.matrices.ls = list()\n",
    "\n",
    "# Store the fitted models for prediction \n",
    "pcox.fit.ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "charged-forty",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(file.path(dir.res.pcox, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/Without_clinical_features/DSS' already exists”\n",
      "Warning message in dir.create(file.path(dir.res.pcox, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/Without_clinical_features/DFI' already exists”\n",
      "Warning message in dir.create(file.path(dir.res.pcox, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/Without_clinical_features/PFI' already exists”\n"
     ]
    }
   ],
   "source": [
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    # Check the number of features\n",
    "    # Regulariation cannot be run if there is only one feature\n",
    "    num.feat = ncol(train_and_validation_ls[[end.point]]$train) - 2\n",
    "    \n",
    "    if (is.null(train_and_validation_ls[[end.point]]$train) == F){\n",
    "        if (num.feat > 1) {\n",
    "    \n",
    "            # Genereate model matrix \n",
    "            model.matrices = generateModelMatrices(train_and_validation_ls[[end.point]]$train, \n",
    "                             train_and_validation_ls[[end.point]]$validation, \n",
    "                             clinical.endpoint = end.point)\n",
    "        \n",
    "            model.matrices.ls[[end.point]] = model.matrices\n",
    "    \n",
    "            # Create output dir \n",
    "            dir.create(file.path(dir.res.pcox, end.point))\n",
    "    \n",
    "            # Find optimal lambda (hyperparameter for elastic net)\n",
    "            pcox.fit = findOptimalLambda(x = model.matrices$x.train.mat, \n",
    "                             y = model.matrices$y.train,\n",
    "                             out.dir = file.path(dir.res.pcox, end.point))\n",
    "        \n",
    "            pcox.fit.ls[[end.point]] = pcox.fit\n",
    "    \n",
    "            # Write the final features included in the model to a file \n",
    "            WriteXLS(pcox.fit$active.k.vals, \n",
    "             file.path(dir.res.pcox, end.point ,\"Active_covariates_in_lambda.min_model.xlsx\"), \n",
    "             BoldHeaderRow = T,\n",
    "             row.names = T)\n",
    "    \n",
    "            # Final significant features \n",
    "            rcox.res.no.clin = pcox.fit$active.k.vals %>% tibble::rownames_to_column(\"Feature\")\n",
    "            rcox.res.no.clin.ls[[end.point]] = rcox.res.no.clin  \n",
    "        } else {\n",
    "            # If no significant features from earlier steps for the clin. end point then store null\n",
    "            model.matrices.ls[[end.point]] = NULL\n",
    "            pcox.fit.ls[[end.point]] = NULL\n",
    "            rcox.res.no.clin.ls[[end.point]] = NULL\n",
    "        }\n",
    "\n",
    "    } else {\n",
    "        # If no significant features from earlier steps for the clin. end point then store null\n",
    "        model.matrices.ls[[end.point]] = NULL\n",
    "        pcox.fit.ls[[end.point]] = NULL\n",
    "        rcox.res.no.clin.ls[[end.point]] = NULL\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-arthur",
   "metadata": {},
   "source": [
    "## 2.3 Make predictions using the cross-validated model and heatmaps for visualisation\n",
    "\n",
    "## 2.3.1 Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "waiting-tribe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the result tables\n",
    "KM.train.by.risk.ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "voluntary-consequence",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    if (!is.null(pcox.fit.ls[[end.point]])) {\n",
    "    \n",
    "        # Predictions for the training set\n",
    "        pred.train <- predict(pcox.fit.ls[[end.point]]$model, \n",
    "                      newx = model.matrices.ls[[end.point]]$x.train.mat, \n",
    "                      s = \"lambda.min\", \n",
    "                      type = \"response\")\n",
    "\n",
    "        # Fitted relative risk\n",
    "        rel.risk <- pred.train[,1] \n",
    "\n",
    "        # Stratify validation data into two groups based on the fitted relative risk\n",
    "        y.data <- as.data.frame(as.matrix(model.matrices.ls[[end.point]]$y.train))\n",
    "        \n",
    "        \n",
    "        # Plot KM and extract the p-value  \n",
    "        KM.train.by.risk = plotKMbyRelativeRisk(data = y.data, \n",
    "                                        rel.risk = rel.risk)\n",
    "        \n",
    "        if (!is.null(KM.train.by.risk)) {\n",
    "        \n",
    "            # Store\n",
    "            KM.train.by.risk.ls[[end.point]] =  KM.train.by.risk$table\n",
    "    \n",
    "            # Store the KM plot\n",
    "            pdf(file.path(dir.res.pcox, end.point ,\"glmnet_K-M_plot_with_training_data.pdf\"), \n",
    "                width = 15, height = 12, onefile = F)\n",
    "            print(KM.train.by.risk$Plot)\n",
    "            dev.off()\n",
    "    \n",
    "            # Heatmap preparation         \n",
    "    \n",
    "            # Variables to be selected \n",
    "            # Because Gender has been changed to a dummy variable its name has been changed\n",
    "            variables.selected = map_chr(rcox.res.no.clin.ls[[end.point]]$Feature, fixVarNames)\n",
    "    \n",
    "            # Get all input variables\n",
    "            heatmap.input.train = model.matrices.ls[[end.point]]$x.train %>% dplyr::select(all_of(variables.selected))\n",
    "    \n",
    "            # Heatmap of training data predictions\n",
    "            hmap.train <- prepareHeatmap(heatmap.input.train, y.data, pred.train, file.path(dir.res.pcox, end.point), \"glmnet_training\", row.height = 8) \n",
    "        \n",
    "        } else {\n",
    "            KM.train.by.risk.ls[[end.point]] = NULL\n",
    "        }\n",
    "        \n",
    "    } else {\n",
    "        KM.train.by.risk.ls[[end.point]] = NULL\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-calculation",
   "metadata": {},
   "source": [
    "## 2.3.2 Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "damaged-apparel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the result tables\n",
    "KM.valid.by.risk.ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "rough-bangkok",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    if (!is.null(pcox.fit.ls[[end.point]])) {\n",
    "    \n",
    "        # Predictions for the training set\n",
    "        pred.valid <- predict(pcox.fit.ls[[end.point]]$model, \n",
    "                      newx = model.matrices.ls[[end.point]]$x.valid.mat, \n",
    "                      s = \"lambda.min\", \n",
    "                      type = \"response\")\n",
    "\n",
    "        # Fitted relative risk\n",
    "        rel.risk <- pred.valid[,1] \n",
    "\n",
    "        # Stratify validation data into two groups based on the fitted relative risk\n",
    "        y.data <- as.data.frame(as.matrix(model.matrices.ls[[end.point]]$y.valid))\n",
    "\n",
    "        # Plot KM and extract the p-value  \n",
    "        KM.valid.by.risk = plotKMbyRelativeRisk(data = y.data, \n",
    "                                        rel.risk = rel.risk)\n",
    "        \n",
    "        if (!is.null(KM.train.by.risk)) {\n",
    "        \n",
    "            # Store\n",
    "            KM.valid.by.risk.ls[[end.point]] =  KM.valid.by.risk$table\n",
    "    \n",
    "            # Store the KM plot\n",
    "            pdf(file.path(dir.res.pcox, end.point ,\"glmnet_K-M_plot_with_validation_data.pdf\"), \n",
    "            width = 15, height = 12, onefile = F)\n",
    "            print(KM.valid.by.risk$Plot)\n",
    "            dev.off()\n",
    "    \n",
    "            # Heatmap preparation \n",
    "    \n",
    "            # Variables to be selected \n",
    "            variables.selected = map_chr(rcox.res.no.clin.ls[[end.point]]$Feature, fixVarNames) \n",
    "    \n",
    "            # Get all input variables\n",
    "            heatmap.input.valid = model.matrices.ls[[end.point]]$x.valid %>% dplyr::select(all_of(variables.selected))\n",
    "    \n",
    "            # Heatmap of training data predictions\n",
    "            hmap.valid <- prepareHeatmap(heatmap.input.valid, y.data, pred.valid, file.path(dir.res.pcox, end.point), \"glmnet_validation\", row.height = 8)  \n",
    "        \n",
    "        } else {\n",
    "            KM.valid.by.risk.ls[[end.point]] = NULL\n",
    "        }\n",
    "        \n",
    "    } else {\n",
    "        KM.valid.by.risk.ls[[end.point]] = NULL\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-session",
   "metadata": {},
   "source": [
    "Merge the two result tables into one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "confirmed-montana",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KM.by.risk.no.clin.train = bind_rows(KM.train.by.risk.ls, .id = \"End point\")\n",
    "KM.by.risk.no.clin.valid = bind_rows(KM.valid.by.risk.ls, .id = \"End point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "exempt-virus",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store final resilts\n",
    "write.csv(KM.by.risk.no.clin.train, file.path(dir.res.pcox, \"Final_evaluation_results_training.csv\"), row.names = F)\n",
    "write.csv(KM.by.risk.no.clin.valid, file.path(dir.res.pcox, \"Final_evaluation_results_validation.csv\"),row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-disposition",
   "metadata": {},
   "source": [
    "# 3 Penalised cox regression with clinical variables\n",
    "\n",
    "In the case of PRAD we will use Age and Gender.\n",
    "\n",
    "Define path to output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "municipal-jefferson",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(dir.res.pcox, recursive = T):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/With_clinical_features' already exists”\n"
     ]
    }
   ],
   "source": [
    "dir.res.pcox = file.path(dir.res.root, \"Penalized_Cox_risk_prediction/customer_features/With_clinical_features\")\n",
    "dir.create(dir.res.pcox, recursive = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-teddy",
   "metadata": {},
   "source": [
    "## 3.1 Preprocess clinical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "institutional-orleans",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function for adding the clinical variables \n",
    "addClinVar = function(data, clin.var) {\n",
    "    if (\"Age\" %in% clin.var) {\n",
    "        data$Age <- data$age_at_diagnosis.clin\n",
    "    } \n",
    "    if (\"Tumor.stage\" %in% clin.var){\n",
    "        data$Tumor.stage = factor(map_chr(data$ajcc_pathologic_stage.clin, reformatTumorStage))\n",
    "    }\n",
    "    if (\"Gender\" %in% clin.var){\n",
    "        data$Gender <- factor(data$gender.clin)    \n",
    "    } \n",
    "    if (\"Gleason.group\" %in% clin.var) {\n",
    "        \n",
    "        # Determine the Gleason group \n",
    "        data$Gleason.group = map2_chr(data$primary_gleason_grade.clin, \n",
    "                                           data$secondary_gleason_grade.clin, \n",
    "                                           determineGleasonGroup)\n",
    "\n",
    "        # Set up the factor levels \n",
    "        data$Gleason.group = factor(data$Gleason.group, \n",
    "                                    levels = c(\"Gleason group 1\", \"Gleason group 2\"))\n",
    "    }\n",
    "    return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "renewable-sleep",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add clinical variables to dataset\n",
    "tcga.dataset = addClinVar(tcga.dataset, clin.var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-collins",
   "metadata": {},
   "source": [
    "## 3.2 Splitting dataset into training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-texas",
   "metadata": {},
   "source": [
    "Generate the final feature ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "continental-better",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we store all the training and validation splits \n",
    "train_and_validation_ls = list()\n",
    "\n",
    "# Number of samples in training and validation cohorts \n",
    "nsamples_step2_ls_with_clin = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "simplified-disease",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 482 cases out of 500 cases\"\n",
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 331 cases out of 500 cases\"\n",
      "[1] \"Taking only complete cases\"\n",
      "[1] \"Including 481 cases out of 500 cases\"\n"
     ]
    }
   ],
   "source": [
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    # We will first combine the significant.features with the outcome variables\n",
    "    # Final list of features \n",
    "    feature.ls = c(paste0(end.point, c(\".clin\",\".time.clin\")), clin.var, significant.features.ls[[end.point]])\n",
    "   \n",
    "    if (is.null(significant.features.ls[[end.point]]) == F) {\n",
    "    \n",
    "        # Now we split the dataset into training and validation cohorts exactly as before.\n",
    "        train_and_validation = splitCases(data = tcga.dataset, \n",
    "                                  split = 0.75, \n",
    "                                  only.complete = T,\n",
    "                                  variables = feature.ls,\n",
    "                                  seed = 42)\n",
    "    \n",
    "        # Store \n",
    "        train_and_validation_ls[[end.point]] = train_and_validation   \n",
    "        nsamples.step2 = c(nrow(train_and_validation$train), \n",
    "                           nrow(train_and_validation$validation))\n",
    "    }    \n",
    "    else {\n",
    "    \n",
    "        # If there are now significant features store NULL\n",
    "        \n",
    "        # Store \n",
    "        train_and_validation_ls[[end.point]] = NULL\n",
    "        nsamples.step2 = c(NA, NA)\n",
    "    }\n",
    "        \n",
    "    names(nsamples.step2) = c(\"ntrain.step2\", \"nvalid.step2\")\n",
    "    nsamples_step2_ls_with_clin[[end.point]] = nsamples.step2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-enlargement",
   "metadata": {},
   "source": [
    "## 3.3 Find the optimal lambda \n",
    "\n",
    "Use 10-fold cross-validation (CV) for the Cox model for different values of lamda. C-index will be use to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "chief-difficulty",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store significant features \n",
    "rcox.res.with.clin.ls = list()\n",
    "\n",
    "# Store model matrices\n",
    "model.matrices.ls = list()\n",
    "\n",
    "# Store the fitted models for prediction \n",
    "pcox.fit.ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "practical-review",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in dir.create(file.path(dir.res.pcox, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/With_clinical_features/DSS' already exists”\n",
      "Warning message in dir.create(file.path(dir.res.pcox, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/With_clinical_features/DFI' already exists”\n",
      "Warning message in dir.create(file.path(dir.res.pcox, end.point)):\n",
      "“'/workstation/project_results/landstrom_project_3/manuscript_work//PRAD/Penalized_Cox_risk_prediction/customer_features/With_clinical_features/PFI' already exists”\n"
     ]
    }
   ],
   "source": [
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    # Check the number of features\n",
    "    # Regulariation cannot be run if there is only one feature\n",
    "    num.feat = ncol(train_and_validation_ls[[end.point]]$train) - 2\n",
    "    \n",
    "    if (is.null(train_and_validation_ls[[end.point]]$train) == F){\n",
    "        if (num.feat > 1) {\n",
    "    \n",
    "            # Genereate model matrix \n",
    "            model.matrices = generateModelMatrices(train_and_validation_ls[[end.point]]$train, \n",
    "                             train_and_validation_ls[[end.point]]$validation, \n",
    "                             clinical.endpoint = end.point)\n",
    "        \n",
    "            model.matrices.ls[[end.point]] = model.matrices\n",
    "    \n",
    "            # Create output dir \n",
    "            dir.create(file.path(dir.res.pcox, end.point))\n",
    "    \n",
    "            # Find optimal lambda (hyperparameter for elastic net)\n",
    "            pcox.fit = findOptimalLambda(x = model.matrices$x.train.mat, \n",
    "                             y = model.matrices$y.train,\n",
    "                             out.dir = file.path(dir.res.pcox, end.point))\n",
    "        \n",
    "            pcox.fit.ls[[end.point]] = pcox.fit\n",
    "    \n",
    "            # Write the final features included in the model to a file \n",
    "            WriteXLS(pcox.fit$active.k.vals, \n",
    "             file.path(dir.res.pcox, end.point ,\"Active_covariates_in_lambda.min_model.xlsx\"), \n",
    "             BoldHeaderRow = T,\n",
    "             row.names = T)\n",
    "    \n",
    "            # Final significant features \n",
    "            rcox.res.with.clin = pcox.fit$active.k.vals %>% tibble::rownames_to_column(\"Feature\")\n",
    "            rcox.res.with.clin.ls[[end.point]] = rcox.res.with.clin  \n",
    "            \n",
    "        } else {\n",
    "            # If no significant features from earlier steps for the clin. end point then store null\n",
    "            model.matrices.ls[[end.point]] = NULL\n",
    "            pcox.fit.ls[[end.point]] = NULL\n",
    "            rcox.res.with.clin.ls[[end.point]] = NULL\n",
    "        }\n",
    "\n",
    "    } else {\n",
    "        # If no significant features from earlier steps for the clin. end point then store null\n",
    "        model.matrices.ls[[end.point]] = NULL\n",
    "        pcox.fit.ls[[end.point]] = NULL\n",
    "        rcox.res.with.clin.ls[[end.point]] = NULL\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-lottery",
   "metadata": {},
   "source": [
    "## 4.4 Make predictions using the cross-validated model\n",
    "\n",
    "## 4.4.1 Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cross-course",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the prediction results \n",
    "training.predictions.ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "going-bench",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set seed \n",
    "set.seed(42)\n",
    "\n",
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    if (!is.null(pcox.fit.ls[[end.point]])) {\n",
    "    \n",
    "        # Predictions for the training set\n",
    "        pred.train <- predict(pcox.fit.ls[[end.point]]$model, \n",
    "                      newx = model.matrices.ls[[end.point]]$x.train.mat, \n",
    "                      s = \"lambda.min\", \n",
    "                      type = \"response\")\n",
    "            \n",
    "        training.predictions.ls[[end.point]] = pred.train\n",
    "    } else {\n",
    "        training.predictions.ls[[end.point]] = NULL\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-motivation",
   "metadata": {},
   "source": [
    "## 4.4.2 Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "tamil-toilet",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the prediction results \n",
    "validation.predictions.ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "respected-evanescence",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "set.seed(42)\n",
    "\n",
    "# Iterate over end points \n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    if (!is.null(pcox.fit.ls[[end.point]])) {\n",
    "    \n",
    "        # Predictions for the validation set\n",
    "        pred.valid <- predict(pcox.fit.ls[[end.point]]$model, \n",
    "                      newx = model.matrices.ls[[end.point]]$x.valid.mat, \n",
    "                      s = \"lambda.min\", \n",
    "                      type = \"response\")\n",
    "\n",
    "        validation.predictions.ls[[end.point]] = pred.valid\n",
    "    } else {\n",
    "        validation.predictions.ls[[end.point]] = NULL\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b78225-c8f2-4b89-9a3c-f959ed078c19",
   "metadata": {},
   "source": [
    "# 5. Divide patients based on the predicted risk and run DE analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438956e-19cd-48ab-b6d0-2d1303d440a1",
   "metadata": {},
   "source": [
    "Output directory : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3b940cb-1962-4e98-82e9-53cba681c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.dir = \"/workstation//project_results/landstrom_core/PRAD_specific_analysis/DE_by_predicted_risk\"\n",
    "dir.create(out.dir, recursive = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8699d04-1589-406b-84ee-b6b1f223f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(DESeq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8084daf8-d0e3-4f34-86c3-6c3e40ca14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.risks.full = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47c76d5f-bd7f-4045-9842-51e2e370cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    predicted.risks.full[[end.point]] = list(\"Risks\" = NULL, \n",
    "                                             \"Median.risk\" = NULL,\n",
    "                                             \"Groups\" = list(\"High.risk\" = NULL , \n",
    "                                                             \"Low.risk\" = NULL))\n",
    "     \n",
    "    valid.risk = validation.predictions.ls[[end.point]]\n",
    "    train.risk = training.predictions.ls[[end.point]]\n",
    "    \n",
    "    if (!is.null(valid.risk)){\n",
    "    \n",
    "        # Merge the predicted risk\n",
    "        predicted.risks.full[[end.point]][[\"Risks\"]] = rbind(train.risk, valid.risk)\n",
    "    \n",
    "        # Calculate median risk\n",
    "        median.risk = median(predicted.risks.full[[end.point]][[\"Risks\"]][,1]) \n",
    "        predicted.risks.full[[end.point]][[\"Median.risk\"]] = median.risk\n",
    "        \n",
    "        # Groups \n",
    "        high.group = rownames(predicted.risks.full[[end.point]][[\"Risks\"]])[predicted.risks.full[[end.point]][[\"Risks\"]] > median.risk]\n",
    "        low.group = rownames(predicted.risks.full[[end.point]][[\"Risks\"]])[predicted.risks.full[[end.point]][[\"Risks\"]] <= median.risk]\n",
    "        predicted.risks.full[[end.point]][[\"Groups\"]][[\"High.risk\"]] = high.group\n",
    "        predicted.risks.full[[end.point]][[\"Groups\"]][[\"Low.risk\"]] = low.group\n",
    "    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c058b61-0c31-4f97-aeab-6f70ffef7326",
   "metadata": {},
   "source": [
    "Prepare the raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bde952a0-4090-45d5-a151-0c11f80f441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>TCGA-HC-A6AN-01A-11R-A30B-07</th><th scope=col>TCGA-G9-6498-01A-12R-A311-07</th><th scope=col>TCGA-J4-AATZ-01A-11R-A41O-07</th><th scope=col>TCGA-CH-5762-01A-11R-1580-07</th><th scope=col>TCGA-G9-A9S7-01A-11R-A41O-07</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>TSPAN6</th><td>3133</td><td>1820</td><td>4651</td><td>3058</td><td>3404</td></tr>\n",
       "\t<tr><th scope=row>TNMD</th><td>   2</td><td>   5</td><td>   0</td><td>   1</td><td>   1</td></tr>\n",
       "\t<tr><th scope=row>DPM1</th><td>1293</td><td>1177</td><td>1875</td><td>1962</td><td>1915</td></tr>\n",
       "\t<tr><th scope=row>SCYL3</th><td> 949</td><td> 474</td><td>1613</td><td>1107</td><td>1401</td></tr>\n",
       "\t<tr><th scope=row>C1orf112</th><td> 183</td><td> 100</td><td> 268</td><td> 281</td><td> 193</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & TCGA-HC-A6AN-01A-11R-A30B-07 & TCGA-G9-6498-01A-12R-A311-07 & TCGA-J4-AATZ-01A-11R-A41O-07 & TCGA-CH-5762-01A-11R-1580-07 & TCGA-G9-A9S7-01A-11R-A41O-07\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tTSPAN6 & 3133 & 1820 & 4651 & 3058 & 3404\\\\\n",
       "\tTNMD &    2 &    5 &    0 &    1 &    1\\\\\n",
       "\tDPM1 & 1293 & 1177 & 1875 & 1962 & 1915\\\\\n",
       "\tSCYL3 &  949 &  474 & 1613 & 1107 & 1401\\\\\n",
       "\tC1orf112 &  183 &  100 &  268 &  281 &  193\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 5\n",
       "\n",
       "| <!--/--> | TCGA-HC-A6AN-01A-11R-A30B-07 &lt;dbl&gt; | TCGA-G9-6498-01A-12R-A311-07 &lt;dbl&gt; | TCGA-J4-AATZ-01A-11R-A41O-07 &lt;dbl&gt; | TCGA-CH-5762-01A-11R-1580-07 &lt;dbl&gt; | TCGA-G9-A9S7-01A-11R-A41O-07 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| TSPAN6 | 3133 | 1820 | 4651 | 3058 | 3404 |\n",
       "| TNMD |    2 |    5 |    0 |    1 |    1 |\n",
       "| DPM1 | 1293 | 1177 | 1875 | 1962 | 1915 |\n",
       "| SCYL3 |  949 |  474 | 1613 | 1107 | 1401 |\n",
       "| C1orf112 |  183 |  100 |  268 |  281 |  193 |\n",
       "\n"
      ],
      "text/plain": [
       "         TCGA-HC-A6AN-01A-11R-A30B-07 TCGA-G9-6498-01A-12R-A311-07\n",
       "TSPAN6   3133                         1820                        \n",
       "TNMD        2                            5                        \n",
       "DPM1     1293                         1177                        \n",
       "SCYL3     949                          474                        \n",
       "C1orf112  183                          100                        \n",
       "         TCGA-J4-AATZ-01A-11R-A41O-07 TCGA-CH-5762-01A-11R-1580-07\n",
       "TSPAN6   4651                         3058                        \n",
       "TNMD        0                            1                        \n",
       "DPM1     1875                         1962                        \n",
       "SCYL3    1613                         1107                        \n",
       "C1orf112  268                          281                        \n",
       "         TCGA-G9-A9S7-01A-11R-A41O-07\n",
       "TSPAN6   3404                        \n",
       "TNMD        1                        \n",
       "DPM1     1915                        \n",
       "SCYL3    1401                        \n",
       "C1orf112  193                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tcga.expr.raw.datamat[1:5,1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a50fd738-8ea9-461d-bda3-b62a0db67b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess raw data\n",
    "raw.data = tcga.expr.raw.datamat\n",
    "colnames(raw.data ) = map_chr(colnames(raw.data), sampleToParticipantID)\n",
    "rownames(raw.data) = paste0(rownames(raw.data), \".exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c467270e-bc9d-4574-ad21-bff1350fc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final raw data expression matrices \n",
    "raw.data.final = list()\n",
    "metadata = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39c95e32-4037-4111-a7cf-9516f74e9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over end points and produce final raw data expression matrices\n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    # Find the subset of samples to be selected \n",
    "    high.risk.group = predicted.risks.full[[end.point]][[\"Groups\"]][[\"High.risk\"]]\n",
    "    low.risk.group = predicted.risks.full[[end.point]][[\"Groups\"]][[\"Low.risk\"]]\n",
    "    \n",
    "    if (!is.null(high.risk.group)){\n",
    "        \n",
    "        # Prepare final raw data for DESeq2 \n",
    "        raw.data.final[[end.point]] = raw.data[,colnames(raw.data) %in% c(high.risk.group, low.risk.group)] \n",
    "    \n",
    "        # Prepare sample metadata for DESeq2\n",
    "        high.risk.group.df = data.frame(\"Condition\" = rep(\"High.risk\", length(high.risk.group )))\n",
    "        rownames(high.risk.group.df) = high.risk.group\n",
    "    \n",
    "        low.risk.group.df = data.frame(\"Condition\" = rep(\"Low.risk\", length(low.risk.group )))\n",
    "        rownames(low.risk.group.df) = low.risk.group\n",
    "    \n",
    "        metadata[[end.point]] = rbind(low.risk.group.df, high.risk.group.df) \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266e928-8bfb-46d2-959b-30a4675be1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "converting counts to integer mode\n",
      "\n",
      "estimating size factors\n",
      "\n",
      "estimating dispersions\n",
      "\n",
      "gene-wise dispersion estimates\n",
      "\n",
      "mean-dispersion relationship\n",
      "\n",
      "final dispersion estimates\n",
      "\n",
      "fitting model and testing\n",
      "\n",
      "-- replacing outliers and refitting for 3389 genes\n",
      "-- DESeq argument 'minReplicatesForReplace' = 7 \n",
      "-- original counts are preserved in counts(dds)\n",
      "\n",
      "estimating dispersions\n",
      "\n",
      "fitting model and testing\n",
      "\n",
      "converting counts to integer mode\n",
      "\n",
      "estimating size factors\n",
      "\n",
      "estimating dispersions\n",
      "\n",
      "gene-wise dispersion estimates\n",
      "\n",
      "mean-dispersion relationship\n",
      "\n",
      "final dispersion estimates\n",
      "\n",
      "fitting model and testing\n",
      "\n",
      "-- replacing outliers and refitting for 3057 genes\n",
      "-- DESeq argument 'minReplicatesForReplace' = 7 \n",
      "-- original counts are preserved in counts(dds)\n",
      "\n",
      "estimating dispersions\n",
      "\n",
      "fitting model and testing\n",
      "\n",
      "converting counts to integer mode\n",
      "\n",
      "estimating size factors\n",
      "\n",
      "estimating dispersions\n",
      "\n",
      "gene-wise dispersion estimates\n",
      "\n",
      "mean-dispersion relationship\n",
      "\n",
      "final dispersion estimates\n",
      "\n",
      "fitting model and testing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over end points and produce final raw data expression matrices\n",
    "for (end.point in c(\"OS\",\"DSS\",\"DFI\",\"PFI\")){\n",
    "    \n",
    "    if (!is.null(metadata[[end.point]])){\n",
    "        \n",
    "        # Get the data  \n",
    "        count.data = raw.data.final[[end.point]] \n",
    "        coldata = metadata[[end.point]] \n",
    "        \n",
    "        # Process \n",
    "        coldata$Condition = factor(coldata$Condition, levels = c(\"Low.risk\",\"High.risk\"))\n",
    "        count.data = count.data[,match(rownames(coldata), colnames(count.data))]\n",
    "        \n",
    "        # Fix the gene names\n",
    "        rownames(count.data) = str_remove(rownames(count.data), \"\\\\.exp\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # Generate DESeq2 object  \n",
    "        dds <- DESeqDataSetFromMatrix(countData = count.data,\n",
    "                              colData = coldata,\n",
    "                              design = ~ Condition)\n",
    "        \n",
    "        # Filter some low expressing genes \n",
    "        keep <- rowSums(counts(dds)) >= 10\n",
    "        dds <- dds[keep,]\n",
    "        \n",
    "        # DE analysis\n",
    "        dds <- DESeq(dds)\n",
    "        \n",
    "        # Get DE results \n",
    "        de.results = as.data.frame(results(dds, lfcThreshold = 0.5))\n",
    "        \n",
    "        # Arrange by P-value \n",
    "        de.results = de.results %>% \n",
    "                           arrange(padj)\n",
    "        \n",
    "        # Write results to csv \n",
    "        write.csv(de.results, file.path(out.dir, paste0(end.point, \"_by_risk_de_results_lfc_05.csv\")))\n",
    "        \n",
    "    }  \n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44e2e4-4bbd-4e64-a593-9f8c5870db6e",
   "metadata": {},
   "source": [
    "# 6. Prepare a volcano plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dbcf1d-35a5-4607-bb75-c8cdbd964af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in results\n",
    "pfi.de.results = read.csv(file.path(out.dir, \"PFI_by_risk_de_results_lfc_05.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0dc36-3004-4c87-b3b3-7a6e5157b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi.de.results$Symbol = pfi.de.results$X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b255909-dd7d-4f93-9581-f022bbadbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi.de.results = pfi.de.results[!(is.na(pfi.de.results$padj)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecdb87-7c7c-4fcc-9b46-64ad5d9dab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which are significant \n",
    "pfi.de.results$Significant = ifelse(pfi.de.results$padj < 0.05 , \"yes\", \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa088539-fb86-48ba-83e1-a6d5f2ccfc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a list of genes to highlight \n",
    "highlight.genes = gene.list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce15864-be4f-4cf9-ab07-51089cd0e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi.de.volcano.plot = ggplot(pfi.de.results, aes(x = log2FoldChange, \n",
    "                                                 y = -1 * log10(pvalue),\n",
    "                                                 colour = Significant)) + \n",
    "                             geom_point() + \n",
    "                             scale_color_manual(values = c(\"yes\" = \"red\", \"no\" = \"#424242\")) +\n",
    "                             #geom_text(data = subset(pfi.de.results, Symbol %in% highlight.genes), aes(label = Symbol), hjust = 0, nudge_x = 0.05) + \n",
    "                             theme_bw() + xlim(-4,4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ca899-d5fd-4d56-a961-6ff0f412b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi.de.volcano.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772298d-3aee-4876-8aff-afa76ffaa096",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggsave(file.path(out.dir, \"PFI_DE_volcanoplot.pdf\"), plot = pfi.de.volcano.plot , width = 8, height = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d49493-0348-45bb-b5a2-c93b5efc40bc",
   "metadata": {},
   "source": [
    "# 7. Pathway enrichment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5507027-da6c-40c3-9fbd-8e873fdfa9fd",
   "metadata": {},
   "source": [
    "## 7.1 ORA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f4d47-a060-4b8d-8c3b-dddd58617d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(clusterProfiler)\n",
    "library(ReactomePA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85858d-b569-48fc-b1ce-0458b2bf8c1f",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b3eb9-481a-4a52-b090-c0884194f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function for converting gene symbols to entrez ids \n",
    "#\n",
    "symbolToEntrez = function(x){\n",
    "    res = bitr(x, fromType=\"SYMBOL\", toType=\"ENTREZID\", OrgDb=\"org.Hs.eg.db\")\n",
    "    colnames(res) = c(\"Feature\", \"ENTREZID\")\n",
    "    return(res)\n",
    "}\n",
    "\n",
    "#\n",
    "# Function for running Overrepresentation analysis against Reactome database \n",
    "#\n",
    "runORAReactome = function(table){\n",
    "    \n",
    "    # Extract the Entrez ids \n",
    "    sig.genes = table$ENTREZID\n",
    "    \n",
    "    # Run ORA\n",
    "    enriched.pathways = enrichPathway(gene=sig.genes, \n",
    "                                      readable=TRUE)\n",
    "    # Return full results\n",
    "    return(enriched.pathways)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9aebe-4de9-4517-afab-d484c68821f2",
   "metadata": {},
   "source": [
    "Read in the DE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee245b-e5d1-4ac9-ac52-fc097971ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read \n",
    "de.results.tables = map(list.files(path = out.dir, pattern = \".csv\", full.names = T) ,read.csv)\n",
    "\n",
    "# Assign names \n",
    "splits = strsplit(list.files(path = out.dir, pattern = \".csv\"), \"_\")\n",
    "names(de.results.tables) = map_chr(splits, function(x){x[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c2fcb-4688-4bb3-acb8-39f15e5d7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "ora.reactome.ls = list()\n",
    "ora.obj.ls = list()\n",
    "logFC.ls = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe57e11-0d3f-4695-93f1-0f29085ba629",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (end.point in names(de.results.tables)){\n",
    "    \n",
    "    # DE results\n",
    "    de.table = de.results.tables[[end.point]]\n",
    "\n",
    "    # Find significant DE genes \n",
    "    diff.genes = de.table %>% \n",
    "                    dplyr::filter(padj < 0.05, baseMean >= 20) \n",
    "\n",
    "    \n",
    "    diff.genes.symbol = as.vector(diff.genes$X)\n",
    "    log.fc = diff.genes$log2FoldChange\n",
    "    names(log.fc) = diff.genes$X\n",
    "    logFC.ls[[end.point]] = log.fc\n",
    "    \n",
    "    # Map symbols to entrez ids \n",
    "    mapped.symbols = symbolToEntrez(diff.genes.symbol)\n",
    "    \n",
    "    # Run ORA\n",
    "    ora.results.obj = runORAReactome(mapped.symbols)\n",
    "    ora.obj.ls[[end.point]] = ora.results.obj\n",
    "    ora.results = ora.results.obj@result\n",
    "    ora.reactome.ls[[end.point]] = ora.results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180e8f4-dd5b-44f4-893d-dc97fb553b05",
   "metadata": {},
   "source": [
    "Write results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdaef35-81e2-4122-b9d2-805a197f010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (end.point in names(ora.reactome.ls)){\n",
    "    write.csv(ora.reactome.ls[[end.point]], file.path(out.dir, paste0(end.point, \"_reactome_pathway_enrichment.csv\")))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15efb41-ddfb-4957-a73d-c74f0aa95834",
   "metadata": {},
   "source": [
    "Check TGFbeta genes in the enriched pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70370c-444e-4fbd-a046-f074061a242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgf_beta_genes = read.csv(\"/workstation//project_data/landstrom_project_3/TGFBeta_genes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e49afb-5e19-4a56-8ae8-4ffdb5656bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "goi = unique(c(tgf_beta_genes$Gene, gene.list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5e855-bf28-4cf2-bd8f-b882a77c9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST if gene in in pathway\n",
    "geneInPath = function(genes.in.path, goi){\n",
    "    genes.in.path.ls = unlist(strsplit(genes.in.path, split = \"/\"))\n",
    "    common.genes = intersect(genes.in.path.ls, goi)\n",
    "    if (length(common.genes) > 0){\n",
    "        return(TRUE)\n",
    "    } else {\n",
    "        return(FALSE)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d9663-7e64-4855-8608-91f40f976b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi.ora.signpathways = ora.reactome.ls$PFI %>% \n",
    "                                filter(p.adjust < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2a264-4130-4fb9-8bfa-17d064b93f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = map_lgl(pfi.ora.signpathways$geneID,\n",
    "       geneInPath,\n",
    "       goi = goi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d461f2-a513-42de-a320-5c02129d9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "top30.pathways.pfi = pfi.ora.signpathways[1:30,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4e75c-4a33-4f9a-8bd8-bc145f48216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top30.pathways.pfi.inc.goi = top30.pathways.pfi[test[1:30],] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048560fc-2e53-4cd0-ae6a-ce1a24a0c957",
   "metadata": {},
   "source": [
    "## 7.2 Visualisations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977d5e4-b368-40fc-9a88-300728b4ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(enrichplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247e165-e04f-408b-9905-2e5e5d6d75c7",
   "metadata": {},
   "source": [
    "### 7.2.1 Dotplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca755f-7466-4399-a483-68e948f8aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment.dotplot = dotplot(ora.obj.ls[[\"PFI\"]], showCategory=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ec133-9fe5-4ef1-8b8d-c1f36776913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment.dotplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69f70d-5a66-4254-b855-112e0c045e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(file.path(out.dir,\"PFI_enrichment_dotplot.pdf\"), width = 10, height = 14)\n",
    "enrichment.dotplot  \n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839f6f2-ba7f-43c5-9792-b9f85b972b32",
   "metadata": {},
   "source": [
    "### 7.2.2 Enrichment plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7b233-21f3-4f74-a5f2-d4cfae613d56",
   "metadata": {},
   "source": [
    "Shows the relatioship of all enriched pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e940ff3-ff72-4922-ba3d-2b1bff378c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ora.obj.ls[[\"PFI\"]] <- pairwise_termsim(ora.obj.ls[[\"PFI\"]])\n",
    "\n",
    "enrichment.plot <- emapplot(ora.obj.ls[[\"PFI\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ac73d-49d2-4b2a-96fd-9e89a533195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6164916-d1a7-42b1-9aa6-73a875f3154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(file.path(out.dir,\"PFI_enrichment_plot.pdf\"), width = 20, height = 20)\n",
    "enrichment.plot \n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca429c-7760-4b48-971a-19a8d02d62af",
   "metadata": {},
   "source": [
    "Seems that the cell cycle related pathways separate as their own module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e9475-91b8-47a5-bbaa-b571bb88c79c",
   "metadata": {},
   "source": [
    "### 7.2.3 Cnet plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30638e8-45e9-4087-92c5-59fe4118191f",
   "metadata": {},
   "source": [
    "Shows the relatioship between genes and pathways. We will show those pathways in top 30 which also include GOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b60e4-f23d-413a-87cf-b24535f1955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ora.obj.ls[[\"PFI\"]] <- setReadable(ora.obj.ls[[\"PFI\"]], 'org.Hs.eg.db', 'ENTREZID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf2165-bab2-4e86-8e93-42c4aec8cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 <- cnetplot(ora.obj.ls[[\"PFI\"]], foldChange=logFC.ls[[\"PFI\"]], \n",
    "               showCategory = top30.pathways.pfi.inc.goi$Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a14257-5d61-4a89-a4ba-16934a31f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(file.path(out.dir,\"cnet_cell_proliferation_pathways.pdf\"), width = 20, height = 20)\n",
    "p1\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b3cb3-bf10-46ac-8c7a-0b8afd5fcce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dabfe7-2a38-4723-be8b-bb7e832fa553",
   "metadata": {},
   "source": [
    "Next we want to produce the same plot but now showing only genes of interest. We include also genes from the \"extended list\" meining the genes belonging to the non-canonical TGFbeta pathway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53178ae7-9683-4708-a597-4200cf8b83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in original gene list \n",
    "original.genes = read.table(\"/workstation/project_data/landstrom_project_3/Customer_genes.tsv\", \n",
    "                            sep = \"\\t\", header = F)\n",
    "\n",
    "# Read in pathway genes\n",
    "tgf_beta_genes = read.csv(\"/workstation//project_data//landstrom_project_3//TGFBeta_genes.csv\")\n",
    "gene.list = c(tgf_beta_genes$Gene, original.genes$V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6e631-c513-4c30-b770-0af81ec136ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = function(x, y){\n",
    "    genes.pathway = unlist(strsplit(x, \"/\"))\n",
    "    intersection = intersect(genes.pathway, y)\n",
    "    return(intersection)\n",
    "}\n",
    "\n",
    "findOverlapsEnr = function(enr.results.df, gene.list){\n",
    "    res = map(enr.results.df$geneID, overlaps, y = gene.list )\n",
    "    names(res) = enr.results.df$Description\n",
    "    return(res)\n",
    "}\n",
    "\n",
    "pathway.overlaps = findOverlapsEnr(top30.pathways.pfi.inc.goi, gene.list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e838b-13d8-47ab-b5b7-ac3387546570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the log-fold changes for these genes \n",
    "genes = unique(unlist(pathway.overlaps))\n",
    "genes.fc = logFC.ls[[\"PFI\"]][genes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819859c5-041c-4527-97dc-3b277f5f831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 <- cnetplot(pathway.overlaps, foldChange = genes.fc, showCategory = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b1f21-d7d4-4d0f-836c-4caaad750850",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf2915-f3c8-4a28-8b35-a5e0939ecf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(file.path(out.dir,\"cnet_plot_top30_with_goi.pdf\"), width = 14, height = 12)\n",
    "p2\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4725dfd-8c1a-4d8b-a366-a25ab0348c2f",
   "metadata": {},
   "source": [
    "Prepare a cnet plot including only pathways with 2 or more GOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce847cd1-bff8-4288-bfff-3b1d25079627",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway.overlaps.cc.2 = pathway.overlaps.cc[unlist(map(pathway.overlaps.cc, length)) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e159c-cee8-497b-9750-195bc1a5095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length(pathway.overlaps.cc.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd522f3-aff3-4637-9d88-a6e7bb0ef82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 <- cnetplot(pathway.overlaps.cc.2, foldChange = genes.fc, showCategory = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801eb8f-cb96-41d3-ae98-e0f83e150e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84431def-c52b-42b3-b2a0-4d631467cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(file.path(out.dir,\"cnet_cell_proliferation_pathways_goi_atleast_2_common.pdf\"), width = 18, height = 14)\n",
    "p3\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3a7b1-4963-42c0-a1db-87b0b525e398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R 4.1.3",
   "language": "R",
   "name": "r4.1.3"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
